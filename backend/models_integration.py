# -*- coding: utf-8 -*-
"""models_integration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L_GuISrFaOfG1v_v90Y5PSMFJemUp3nL

## Import libraries
"""

import numpy as np
import joblib
import os
import pandas as pd

import torch
import torch.nn as nn


import os

# PATH SETUP
# Get directory of the current script
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))

# Path to exported_models in the same folder
MODEL_PATH = os.path.join(CURRENT_DIR, 'exported_models')

# LSTM hyperparameters JSON path
HYPERPARAM_JSON_PATH = os.path.join(MODEL_PATH, 'lstm_hyperparams.json')

# Path to customer dataset (to get customer clusters)
CUSTOMER_DATASET_PATH = os.path.join(CURRENT_DIR, 'datasets', 'customer_information_engineered_kMeans.csv')
ASSET_DATASET_PATH = os.path.join(CURRENT_DIR, 'datasets', 'asset_information_with_engineered.csv')

ARIMA_COVARIANCE_PATH = os.path.join(CURRENT_DIR, 'datasets', 'processed_data', 'covariance.csv')
ARIMA_PREDICTIONS_PATH = os.path.join(CURRENT_DIR, 'datasets', 'processed_data', 'predictions.csv')
CLOSE_PRICES_PATH = os.path.join(CURRENT_DIR, 'datasets', 'close_prices.csv')


# load asset dataset once to build mapping
asset_df = pd.read_csv(ASSET_DATASET_PATH)
isin_to_name = dict(zip(asset_df['ISIN'], asset_df['assetName']))


"""## Loading exported

### 1. HybridNCF
"""

import os
import numpy as np
import joblib
import torch
import torch.nn as nn

def setup_ncf_pipeline(model_path):
    """
    Loads all necessary arrays, mappings, and models for NCF inference.

    Returns:
        dict with keys:
        - user_feat_array
        - asset_feat_array
        - user_id_to_index
        - asset_id_to_index
        - asset_id_to_isin
        - bce_model
        - bpr_model
    """

    # ---------------------
    # 1. Feature arrays & mappings
    # ---------------------
    user_feat_array = np.load(os.path.join(model_path, 'user_feat_array.npy'))
    asset_feat_array = np.load(os.path.join(model_path, 'asset_feat_array.npy'))

    user_id_to_index = joblib.load(os.path.join(model_path, 'user_id_to_index.joblib'))
    asset_id_to_index = joblib.load(os.path.join(model_path, 'asset_id_to_index.joblib'))
    asset_id_to_isin = joblib.load(os.path.join(model_path, 'asset_id_to_isin.joblib'))

    num_users = user_feat_array.shape[0]
    num_assets = asset_feat_array.shape[0]
    user_feat_dim = user_feat_array.shape[1]
    asset_feat_dim = asset_feat_array.shape[1]

    # ---------------------
    # 2. Model class
    # ---------------------
    class HybridNCF(nn.Module):
        def __init__(self, num_users, num_assets, user_feat_dim, asset_feat_dim,
                    emb_dim=32, mlp_layers=[128, 64], dropout=0.2, activation='relu'):
            super().__init__()
            self.user_emb = nn.Embedding(num_users, emb_dim)
            self.asset_emb = nn.Embedding(num_assets, emb_dim)
            self.user_fc = nn.Linear(user_feat_dim, emb_dim)
            self.asset_fc = nn.Linear(asset_feat_dim, emb_dim)

            act = nn.ReLU() if activation == 'relu' else nn.LeakyReLU(0.1)

            # dynamically build MLP layers
            layers = []
            input_dim = emb_dim * 4
            for h_dim in mlp_layers:
                layers += [nn.Linear(input_dim, h_dim), act, nn.Dropout(dropout)]
                input_dim = h_dim
            layers.append(nn.Linear(input_dim, 1))
            self.mlp = nn.Sequential(*layers)

            self._init_weights()

        def forward(self, user_id, asset_id, user_feat, asset_feat):
            u_emb = self.user_emb(user_id)
            a_emb = self.asset_emb(asset_id)
            u_feat = self.user_fc(user_feat)
            a_feat = self.asset_fc(asset_feat)
            x = torch.cat([u_emb, a_emb, u_feat, a_feat], dim=1)
            return self.mlp(x).squeeze(1)

        def _init_weights(self):
            for m in self.modules():
                if isinstance(m, nn.Linear):
                    nn.init.xavier_uniform_(m.weight)
                    nn.init.zeros_(m.bias)
                elif isinstance(m, nn.Embedding):
                    nn.init.normal_(m.weight, mean=0, std=0.01)

    # ---------------------
    # 3. Load models
    # ---------------------
    # bce_model = HybridNCF(num_users, num_assets, user_feat_dim, asset_feat_dim)
    # bce_model.load_state_dict(torch.load(os.path.join(model_path, 'ncf_bce.pth')))
    # bce_model.eval()

    bpr_model = HybridNCF(num_users, num_assets, user_feat_dim, asset_feat_dim)
    bpr_model.load_state_dict(torch.load(os.path.join(model_path, 'ncf_bpr.pth')))
    bpr_model.eval()

    # ---------------------
    # 4. Return everything
    # ---------------------
    return {
        'user_feat_array': user_feat_array,
        'asset_feat_array': asset_feat_array,
        'user_id_to_index': user_id_to_index,
        'asset_id_to_index': asset_id_to_index,
        'asset_id_to_isin': asset_id_to_isin,
        'bpr_model': bpr_model,
        'HybridNCF_class': HybridNCF
    }

import torch
import torch.nn.functional as F
import numpy as np

def recommend_assets_ncf(customer_id, model, customer_to_userid,
                         user_id_to_index, asset_id_to_index,
                         user_feat_array, asset_feat_array,
                         asset_id_to_isin, topk=10, existing_portfolio=None):
    if customer_id not in customer_to_userid:
        return f"Customer {customer_id} not found."

    user_id = customer_to_userid[customer_id]
    u_idx = user_id_to_index[user_id]
    u_feat = torch.FloatTensor(user_feat_array[u_idx]).unsqueeze(0)

    all_asset_ids = np.array(list(asset_id_to_index.keys()))
    a_feat = torch.FloatTensor(asset_feat_array[[asset_id_to_index[i] for i in all_asset_ids]])

    u_ids = torch.LongTensor([user_id]*len(all_asset_ids))
    u_feats_batch = u_feat.repeat(len(all_asset_ids), 1)

    model.eval()
    with torch.no_grad():
        logits = model(u_ids, torch.LongTensor(all_asset_ids), u_feats_batch, a_feat)
        scores = torch.sigmoid(logits).numpy()  # convert to probability [0,1]

    topk_idx = np.argsort(-scores)

    recommended = []
    existing_set = set(existing_portfolio) if existing_portfolio else set()

    for i in topk_idx:
        isin = asset_id_to_isin[all_asset_ids[i]]
        if isin in existing_set:
            continue
        recommended.append((isin, float(scores[i])))  # float for JSON-friendly format
        if len(recommended) >= topk:
            break

    # Return dictionary {ISIN: score}
    return {isin: score for isin, score in recommended}



""" 
2. LSTM
"""
import json

def setup_lstm_pipeline(model_path, hyperparam_json_path):

    # ---------------------
    # 1. Load mappings
    # ---------------------
    lstm_isin_to_idx = joblib.load(os.path.join(model_path, 'lstm_isin_to_idx.pkl'))
    lstm_idx_to_isin = joblib.load(os.path.join(model_path, 'lstm_idx_to_isin.pkl'))
    lstm_customer_to_idx = joblib.load(os.path.join(model_path, 'lstm_customer_to_idx.pkl'))

    # ---------------------
    # 2. Load hyperparameters
    # ---------------------
    with open(hyperparam_json_path, 'r') as f:
        hyperparams = json.load(f)

    embedding_dim = hyperparams.get("embedding_dim")
    hidden_dim = hyperparams["hidden_dim"]
    num_assets = hyperparams.get("num_assets")
    num_customers = hyperparams.get("num_customers")
    output_size = hyperparams.get("output_size")
    window_size = hyperparams.get("window_size")
    padding_idx = hyperparams.get("padding_idx", 0)

    # --------------------
    # 3. Model class
    # --------------------
    class LSTMRecommender(nn.Module):
        def __init__(self, embedding_dim, hidden_dim, num_assets, num_customers, output_size, padding_idx=0):
            super(LSTMRecommender, self).__init__()
            self.asset_embedding = nn.Embedding(num_assets, embedding_dim, padding_idx=padding_idx)
            self.customer_embedding = nn.Embedding(num_customers, embedding_dim)
            self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
            self.fc = nn.Linear(hidden_dim + embedding_dim, output_size)

        def forward(self, input_seq, customer_id):
            asset_embedded = self.asset_embedding(input_seq)
            lstm_out, (h_n, c_n) = self.lstm(asset_embedded)
            last_hidden = h_n[-1]

            customer_embedded = self.customer_embedding(customer_id)
            combined = torch.cat([last_hidden, customer_embedded], dim=1)
            output = self.fc(combined)
            return output

    # ---------------------
    # 4. Load model
    # ---------------------
    lstm_model = LSTMRecommender(embedding_dim, hidden_dim, num_assets, num_customers, output_size, padding_idx)
    lstm_model.load_state_dict(torch.load(os.path.join(model_path, 'lstm.pth')))
    lstm_model.eval()

    # ---------------------
    # 5. Return everything
    # ---------------------
    return {
        'lstm_isin_to_idx': lstm_isin_to_idx,
        'lstm_idx_to_isin': lstm_idx_to_isin,
        'lstm_customer_to_idx': lstm_customer_to_idx,
        'embedding_dim': embedding_dim,
        'hidden_dim': hidden_dim,
        'num_assets': num_assets,
        'num_customers': num_customers,
        'output_size': output_size,
        'window_size': window_size,
        'padding_idx': padding_idx,
        'lstm_model': lstm_model,
        'lstm_recommender_class': LSTMRecommender
    }

import torch
import torch.nn.functional as F
import numpy as np


#existing portfolio gonna be all the buy transactions?


def recommend_assets_lstm (customer_id, customer_to_idx,
                           model,
                           isin_to_idx, idx_to_isin,
                           topk=10,
                           window_size=3,
                           padding_idx=0,
                           existing_portfolio=None):
  if customer_id not in customer_to_idx:
    return f"Customer {customer_id} not found."

  customer_idx = torch.tensor([customer_to_idx[customer_id]], dtype=torch.long)

  if not existing_portfolio or len(existing_portfolio) == 0: #safest if its recent buys from transactions
    seq_indices = [padding_idx] * window_size
  else:
    seq_indices = [isin_to_idx[isin] for isin in existing_portfolio[-window_size:]]
    if len(seq_indices) < window_size:
      seq_indices = [padding_idx] * (window_size - len(seq_indices)) + seq_indices
    else:
      seq_indices = seq_indices[-window_size:]

  seq_tensor = torch.tensor(seq_indices, dtype=torch.long).unsqueeze(0)

  with torch.no_grad():
    model.eval()
    logit_scores = model(seq_tensor, customer_idx) # shape: (1, num_assets)
    probs = torch.sigmoid(logit_scores).squeeze(0) # shape: (num_assets), change to prob btw 0-1

  # Sort by descending probability
  top_idx = np.argsort(-probs) #list of tensors
  top_idx = top_idx.tolist() #convert to list of intergers else fails for loop
  recommended = []
  existing_set = set(existing_portfolio) if existing_portfolio else set()

  for i in top_idx:
    #print(i)
    isin = idx_to_isin.get(i + 1) #idx_to_isin starts from 1, but system indices start from 0
    if not isin or isin in existing_set:
      continue
    recommended.append((isin, float(probs[i])))  # (ISIN, probability)
    if len(recommended) >= topk:
        break


  #print(f"Top idx: {top_idx[:10]}")
  #print(idx_to_isin.get(256))
  #print(idx_to_isin.get(1))

  return {isin: score for isin, score in recommended}



"""
3. Content based filtering
"""

def setup_content_based_pipeline(model_path):
    # ---------------------
    # 1. Load the csvs
    # ---------------------
    item_matrix_csv = os.path.join(model_path, "item_matrix_cb.csv")
    user_profiles_csv = os.path.join(model_path, "user_profiles_cb.csv")

    # ---------------------
    # 1. Load the dfs
    # ---------------------
    item_matrix_df = pd.read_csv(item_matrix_csv)
    user_profiles_df = pd.read_csv(user_profiles_csv) #contains for customers in cluster 0 and 2 only

    return {
        'item_matrix_df': item_matrix_df,
        'user_profiles_df': user_profiles_df,
    }

from sklearn.metrics.pairwise import cosine_similarity

def recommend_assets_cb(customer_id, item_matrix_df, user_profiles_df, topk=10, existing_portfolio=None):
    customer_id = customer_id.strip()
    if customer_id not in user_profiles_df['customerID'].values:
        return f"Customer {customer_id} not found"

    customer_profile = user_profiles_df[user_profiles_df['customerID'] == customer_id]

    feature_cols = [col for col in item_matrix_df.columns if col != 'ISIN']
    user_features = customer_profile[feature_cols].values
    item_features = item_matrix_df[feature_cols].values

    similarity_scores = cosine_similarity(user_features, item_features).flatten()  # shape (num_items,)

    sorted_indices = np.argsort(similarity_scores)[::-1]

    recommended = {}
    for idx in sorted_indices:
        isin = item_matrix_df['ISIN'].iloc[idx]
        score = float(similarity_scores[idx])

        # Skip if in existing portfolio
        if existing_portfolio and isin in existing_portfolio:
            continue

        recommended[isin] = score
        if len(recommended) >= topk:
            break

    return recommended


"""Getting CustomerID and cluster   """

customers_df = pd.read_csv(CUSTOMER_DATASET_PATH)
customer_clusters_df = customers_df[['customerID', 'cluster']].copy()


from typing import Optional

def forecast_sharpe_ratio(
    isin: str,
    predictions_path: Optional[str],
    covariance_path: Optional[str],
    close_prices_path: Optional[str],
) -> float:
    """
    Compute the Sharpe ratio for a given ISIN using stored price forecasts and the covariance matrix.

    Args:
        isin: Asset identifier to evaluate.
        predictions_path: Optional path override for ``predictions.csv``.
        covariance_path: Optional path override for ``covariance.csv``.
        close_prices_path: Optional path override for ``close_prices.csv``.

    Returns:
        The Sharpe ratio derived from the mean of predicted returns and the asset's standard deviation.
    """
    if not isin:
        raise ValueError("isin must be provided.")


    # current_dir = os.path.dirname(os.path.abspath(__file__))
    # datasets_dir = os.path.join(current_dir, "..", "datasets")
    predictions_path = predictions_path # or os.path.join(datasets_dir, "predictions.csv")
    covariance_path = covariance_path # or os.path.join(datasets_dir, "covariance.csv")
    close_prices_path = close_prices_path # or os.path.join(datasets_dir, "close_prices.csv")

    if not os.path.exists(predictions_path):
        raise FileNotFoundError(f"Predictions file not found at {predictions_path}.")

    predictions_df = pd.read_csv(predictions_path)
    predictions_df["ISIN"] = predictions_df["ISIN"].astype(str)
    print(f"Predictions CSV columns: {predictions_df.columns.tolist()}")

    isin_str = str(isin)
    asset_predictions = predictions_df[predictions_df["ISIN"] == isin_str].copy()
    if asset_predictions.empty:
        raise ValueError(f"No predictions available for ISIN {isin_str}.")

    asset_predictions["timestamp"] = pd.to_datetime(asset_predictions["timestamp"])
    asset_predictions = asset_predictions.sort_values("timestamp")
    predicted_prices = pd.to_numeric(asset_predictions["closePrice"], errors="coerce").dropna()
    if predicted_prices.empty:
        raise ValueError(f"Predictions for ISIN {isin_str} contain no valid price values.")

    if not os.path.exists(close_prices_path):
        raise FileNotFoundError(f"Close price dataset not found at {close_prices_path}.")

    close_prices_df = pd.read_csv(close_prices_path)
    close_prices_df["ISIN"] = close_prices_df["ISIN"].astype(str)
    asset_history = close_prices_df.loc[close_prices_df["ISIN"] == isin_str, ["timestamp", "closePrice"]]
    if asset_history.empty:
        raise ValueError(f"No historical prices found for ISIN {isin_str}.")

    asset_history["timestamp"] = pd.to_datetime(asset_history["timestamp"])
    asset_history = asset_history.sort_values("timestamp")
    historical_prices = pd.to_numeric(asset_history["closePrice"], errors="coerce").dropna()
    if historical_prices.empty:
        raise ValueError(f"No numeric historical prices found for ISIN {isin_str}.")

    base_price = float(historical_prices.iloc[-1])
    augmented_prices = pd.concat(
        [pd.Series([base_price], dtype=float), predicted_prices.reset_index(drop=True)],
        ignore_index=True,
    )
    predicted_returns = augmented_prices.pct_change().dropna()
    mean_return = float(predicted_returns.mean()) if not predicted_returns.empty else 0.0

    covariance_df = pd.read_csv(covariance_path, index_col=0)
    if isin_str not in covariance_df.index or isin_str not in covariance_df.columns:
        raise ValueError(f"ISIN {isin_str} not found in covariance matrix.")

    variance = float(covariance_df.loc[isin_str, isin_str])
    variance = max(variance, 0.0)
    std_dev = float(np.sqrt(variance))

    if np.isclose(std_dev, 0.0):
        if mean_return > 0:
            return 1.0
        if mean_return < 0:
            return -1.0
        return 0.0

    return float(mean_return / std_dev)

"""### Getting recommendations from all models"""

# WHEN USER PRESSES "GET RECOMMENDATIONS" -> CALLS RECO()


def get_cluster(customerID):
    # returns a string, "whales" OR "cores" OR "browsers"
    cluster_map = {0: "whales", 1: "browsers", 2: "cores"}
    row = customer_clusters_df.loc[
        customer_clusters_df['customerID'] == customerID, 'cluster'
    ]
    if row.empty:
        return None
    return cluster_map[int(row.iloc[0])]

def run_content_based(customerID, existing_portfolio):
    # returns a dictionary of {stock1: score, stock2: score,..., stock20: score}
    pipeline_cb = setup_content_based_pipeline(MODEL_PATH)

    item_matrix_df = pipeline_cb['item_matrix_df']
    user_profiles_df = pipeline_cb['user_profiles_df']

    # call content based recommend function
    recommended_assets_cb = recommend_assets_cb(
        customer_id=customerID,
        item_matrix_df=item_matrix_df,
        user_profiles_df=user_profiles_df,
        topk=10,
        existing_portfolio=existing_portfolio   # optional
    )

    return recommended_assets_cb

def run_ncf(customerID, existing_portfolio):
    # returns a dictionary of {stock1: score, stock2: score,..., stock20: score}
    ncf_customer_to_userid = joblib.load(os.path.join(MODEL_PATH, 'customer_to_userid.joblib'))
    pipeline = setup_ncf_pipeline(MODEL_PATH)
    ncf_recs = recommend_assets_ncf(
                customer_id=customerID,
                model=pipeline['bpr_model'],  # can also use pipeline['bpr_model']
                customer_to_userid=ncf_customer_to_userid,
                user_id_to_index=pipeline['user_id_to_index'],
                asset_id_to_index=pipeline['asset_id_to_index'],
                user_feat_array=pipeline['user_feat_array'],
                asset_feat_array=pipeline['asset_feat_array'],
                asset_id_to_isin=pipeline['asset_id_to_isin'],
                topk=10,
                existing_portfolio=existing_portfolio
            )
    return ncf_recs

def run_lstm(customerID, existing_portfolio):
    # returns a dictionary of {stock1: score, stock2: score,..., stock20: score}
    # MUST CHANGE

    pipeline_lstm = setup_lstm_pipeline(MODEL_PATH, HYPERPARAM_JSON_PATH)

    # map customer ID to index
    lstm_customer_to_idx = pipeline_lstm['lstm_customer_to_idx']
    lstm_isin_to_idx = pipeline_lstm['lstm_isin_to_idx']
    lstm_idx_to_isin = pipeline_lstm['lstm_idx_to_isin']
    top_10 = recommend_assets_lstm(
          customer_id=customerID,
          customer_to_idx=lstm_customer_to_idx,
          model=pipeline_lstm['lstm_model'],
          isin_to_idx=lstm_isin_to_idx,
          idx_to_isin=lstm_idx_to_isin,
          topk=10,
          window_size=pipeline_lstm['window_size'],
          padding_idx=0,
          existing_portfolio=existing_portfolio
      )
    return top_10

def run_arima(stock):
    return forecast_sharpe_ratio(
      isin = stock,
      predictions_path = ARIMA_PREDICTIONS_PATH,
      covariance_path = ARIMA_COVARIANCE_PATH,
      close_prices_path = CLOSE_PRICES_PATH,
    )

def get_top_10(dicts):
    n = len(dicts) # number of models, to divide by n to calculate weighted avg similarity score
    new = {}
    # print(dicts)
    for d in dicts:
        for stock, score in d.items():
            if stock in new:
                new[stock] += score / n
            else:
                new[stock] = score / n

    # rank stocks in new (need to convert to list bc cannot sort a dictionary)
    new_list = [] # [[stock, score], [stock, score],...]
    for stock, total_score in new.items():
        new_record = [stock, total_score]
        new_list.append(new_record)
    new_list.sort(key=lambda x: x[1], reverse=True)

    # return only first 10 stocks (with weighted avg similarity score)
    return new_list[:10]


# def top_10_past(customerID, existing_portfolio):
#     """
#     Depending on user's cluster, runs relevant models and outputs top 10
#     for ARIMA ranking. Also keeps track of which model produced which dict.
#     Skips any model that fails or returns invalid output.
#     """
#     dicts = []

#     cluster = get_cluster(customerID)

#     # Define model functions and names for this customer
#     if cluster == "cores":  # neural + lstm
#         model_funcs = [("ncf", run_ncf), ("lstm", run_lstm)]
#     else:  # whales/browsers -> cb + lstm
#         model_funcs = [("cb", run_content_based), ("lstm", run_lstm)]

#     for model_name, func in model_funcs:
#         try:
#             output = func(customerID, existing_portfolio)
#             if isinstance(output, dict) and output:  # valid dict with content
#                 dicts.append((model_name, output))
#             else:
#                 print(f"Skipping {model_name}: invalid or empty output for customer {customerID}")
#         except Exception as e:
#             print(f"Error running {model_name} for customer {customerID}: {e}")

#     if not dicts:
#         print(f"No valid recommendations for customer {customerID}")
#         return []

#     # DEBUG: print which model produced which scores
#     for model_name, d in dicts:
#         print(f"Model: {model_name}, Scores: {d}")

#     # Combine the dicts for weighted top 10
#     top_10 = get_top_10([d for _, d in dicts])
#     print(f"Weighted top_10: {top_10}")

#     return top_10

def top_10_past(customerID, existing_portfolio):
    dicts = []
    not_found_msgs = []

    cluster = get_cluster(customerID)
    if cluster == "cores":
        model_funcs = [("ncf", run_ncf), ("lstm", run_lstm)]
    else:
        model_funcs = [("cb", run_content_based), ("lstm", run_lstm)]

    for model_name, func in model_funcs:
        try:
            output = func(customerID, existing_portfolio)
            if isinstance(output, dict) and output:  # valid dict
                dicts.append((model_name, output))
            elif isinstance(output, str) and "not found" in output.lower():
                not_found_msgs.append(f"{model_name}: {output}")
            else:
                print(f"Skipping {model_name}: invalid output for customer {customerID}")
        except Exception as e:
            print(f"Error running {model_name} for customer {customerID}: {e}")

    if not dicts:
        msg = "; ".join(not_found_msgs) if not_found_msgs else "No valid recommendations."
        raise ValueError(msg)

    top_10 = get_top_10([d for _, d in dicts])
    return top_10


def top_10_future_ranked(customerID, existing_portfolio):
    # stock is ISIN
    # returns [[stock, score, sharpe], [stock, score, sharpe],...] based on descending sharpe
    past10 = top_10_past(customerID, existing_portfolio) # [[stock1, score1], [stock2, score2], ..., [stock10, score10]]
    new_list = []
    for isin, score in past10:
        sharpe = run_arima(isin) # calls arima on the stock name
        assetName = isin_to_name.get(isin, None)
        new_list.append([isin, assetName, score, sharpe]) # new_list = [[stock, score, sharpe], [stock, score, sharpe]...]

    new_list.sort(key=lambda x: x[3], reverse=True) # sort by sharpe descending
    print(new_list)
    return new_list

def recommend(customerID, existing_portfolio):
    return top_10_future_ranked(customerID, existing_portfolio)
    # then see what we wna do with the entries (if we wna display the weighted avg similarity score / sharpe from arima)
